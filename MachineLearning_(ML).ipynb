{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Machine Learning**\n",
        "\n",
        "## **1. What is Machine Learning, and how does it differ from traditional programming?**\n",
        "\n",
        "- **Machine Learning (ML)** is a subset of artificial intelligence that allows computers to learn patterns from data and make decisions without being explicitly programmed.  \n",
        "\n",
        "### **Differences:**\n",
        "| Aspect | Traditional Programming | Machine Learning |\n",
        "|--------|------------------------|-----------------|\n",
        "| **Approach** | Uses predefined rules and logic | Learns from data and improves over time |\n",
        "| **Example** | IF-ELSE conditions to detect spam emails | Uses a spam filter trained on past spam messages |\n",
        "| **Adaptability** | Fixed rules | Improves with more data |\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How is Machine Learning applied in e-commerce applications?**\n",
        "\n",
        "- ML is widely used in e-commerce for:\n",
        "\n",
        " - **Product Recommendations:** Amazon, Flipkart suggest items based on past purchases.\n",
        "\n",
        " - **Fraud Detection:** Identifies suspicious transactions in payments.\n",
        "\n",
        " - **Customer Segmentation:** Groups customers for targeted marketing.\n",
        "\n",
        " - **Chatbots & Virtual Assistants:** AI-powered customer support.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. What are some common algorithms used in Machine Learning?**\n",
        "\n",
        "- **Supervised Learning:**\n",
        "\n",
        " - Regression: **Linear Regression, Decision Trees**\n",
        "\n",
        " - Classification: **Logistic Regression, Random Forest, SVM**\n",
        "\n",
        "- **Unsupervised Learning:**\n",
        "\n",
        " - Clustering: **K-Means, DBSCAN**\n",
        " - Dimensionality Reduction: **PCA, t-SNE**\n",
        "\n",
        "- **Reinforcement Learning:**\n",
        "\n",
        " - **Q-Learning, Deep Q Networks (DQN)**\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Describe the typical workflow of a Machine Learning project.**\n",
        "\n",
        "1. **Define the Problem**\n",
        "\n",
        "2. **Collect & Preprocess Data**\n",
        "\n",
        "3. **Select Features & Train Model**\n",
        "\n",
        "4. **Evaluate & Tune Model**\n",
        "\n",
        "5. **Deploy Model for real-world use**\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ AI vs ML vs DL vs DS**\n",
        "\n",
        "## **5. Key differences between AI, ML, DL, and Data Science**\n",
        "\n",
        "| Concept | Definition |\n",
        "|---------|------------|\n",
        "| **Artificial Intelligence (AI)** | The broader concept of machines mimicking human intelligence |\n",
        "| **Machine Learning (ML)** | A subset of AI that learns from data |\n",
        "| **Deep Learning (DL)** | A subset of ML using neural networks |\n",
        "| **Data Science (DS)** | The field that analyzes and interprets data, often using ML |\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Example where AI is applied but not ML, and ML is applied but not DL**\n",
        "\n",
        "- **AI without ML:** Rule-based chatbots, expert systems.\n",
        "\n",
        "- **ML without DL:** Spam email detection using Logistic\n",
        "Regression.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Subfields of AI closely related to ML**\n",
        "\n",
        "- **Natural Language Processing (NLP)**\n",
        "\n",
        "- **Computer Vision**\n",
        "\n",
        "- **Robotics**\n",
        "\n",
        "- **Recommendation Systems**\n",
        "\n",
        "---\n",
        "\n",
        "## **8. How can deep learning improve machine learning tasks?**\n",
        "\n",
        "- **Automatically learns complex patterns** without manual feature engineering.\n",
        "\n",
        "- **Performs well on unstructured data (images, text, audio).**\n",
        "\n",
        "- **Scales better with large datasets** using neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Types of Machine Learning**\n",
        "\n",
        "## **9. What are the main types of Machine Learning?**\n",
        "\n",
        "- **Supervised Learning** â€“ Labeled data (Example: Email spam detection)\n",
        "\n",
        "- **Unsupervised Learning** â€“ No labels (Example: Customer segmentation)\n",
        "\n",
        "- **Reinforcement Learning** â€“ Reward-based learning (Example: Game-playing AI)\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Difference between supervised and unsupervised learning**\n",
        "\n",
        "| Type | Supervised Learning | Unsupervised Learning |\n",
        "|------|--------------------|----------------------|\n",
        "| **Labeled Data** | Yes | No |\n",
        "| **Example** | Spam Detection | Customer Segmentation |\n",
        "\n",
        "---\n",
        "\n",
        "## **11. What is reinforcement learning?**\n",
        "\n",
        "- **Learns by trial and error.**\n",
        "\n",
        "- **Uses rewards and penalties to optimize decisions.**\n",
        "\n",
        "- **Example:** Training an AI to play chess.\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Data Preprocessing**\n",
        "\n",
        "## **12. Why split data into training, testing, and validation sets?**\n",
        "\n",
        "- **Training Set:** Used to train the model.\n",
        "\n",
        "- **Validation Set:** Tunes hyperparameters.\n",
        "\n",
        "- **Test Set:** Evaluates final performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **13. What is cross-validation?**\n",
        "\n",
        "- **Divides data into multiple train-test splits.**\n",
        "\n",
        "- **Ensures model performance is not biased.**\n",
        "\n",
        "- **Common type:** **K-Fold Cross-Validation.**\n",
        "\n",
        "---\n",
        "\n",
        "## **14. What is data leakage?**\n",
        "\n",
        "- **Occurs when information from the test set influences the\n",
        "model during training.**\n",
        "\n",
        "- **Leads to over-optimistic results.**\n",
        "\n",
        "- **Example:** Using future stock prices as features.\n",
        "\n",
        "---\n",
        "\n",
        "## **15. Choosing the right size for training, validation, and test sets**\n",
        "\n",
        "| Dataset | Typical Split |\n",
        "|---------|--------------|\n",
        "| **Training Set** | 70-80% |\n",
        "| **Validation Set** | 10-15% |\n",
        "| **Test Set** | 10-15% |\n",
        "\n",
        "---\n",
        "\n",
        "## **16. K-Fold Cross-Validation vs Standard Train-Test Split**\n",
        "\n",
        "- **K-Fold Cross-Validation**: Divides data into **K** parts and trains **K** times.\n",
        "\n",
        "- **Standard Train-Test Split**: Splits data once into training and test sets.\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Overfitting, Underfitting, Bias-Variance**\n",
        "\n",
        "## **17. What is overfitting, and how to prevent it?**\n",
        "\n",
        "- **Model learns noise instead of patterns.**\n",
        "\n",
        "- **Prevention:** Regularization, pruning, dropout, early stopping.\n",
        "\n",
        "---\n",
        "\n",
        "## **18. What is underfitting?**\n",
        "\n",
        "- **Model is too simple to learn patterns.**\n",
        "\n",
        "- **Example:** Linear model for non-linear data.\n",
        "\n",
        "---\n",
        "\n",
        "## **19. Bias-Variance Tradeoff**\n",
        "\n",
        "- **High Bias:** Underfitting\n",
        "\n",
        "- **High Variance:** Overfitting\n",
        "\n",
        "---\n",
        "\n",
        "## **20. How does regularization prevent overfitting?**\n",
        "\n",
        "- **L1 Regularization (Lasso)** removes less important features.\n",
        "\n",
        "- **L2 Regularization (Ridge)** shrinks feature importance.\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Handling Missing Data**\n",
        "## **21. Techniques for handling missing data**\n",
        "\n",
        "- **Remove missing values**\n",
        "\n",
        "- **Imputation (Mean, Median, Mode)**\n",
        "\n",
        "- **KNN-based imputation**\n",
        "\n",
        "---\n",
        "\n",
        "## **22. Example: Handling missing data in Python**\n",
        "\n",
        "```python\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1, np.nan, 2], [3, 4, np.nan], [5, 6, 7]])\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data_imputed = imputer.fit_transform(data)\n",
        "print(data_imputed)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Handling Imbalanced Data**\n",
        "\n",
        "## **23. Challenges of imbalanced datasets**\n",
        "\n",
        "- **Biased model predictions** (favoring majority class).\n",
        "\n",
        "- **Poor generalization on minority class.**\n",
        "\n",
        "---\n",
        "\n",
        "## **24. What is SMOTE?**\n",
        "\n",
        "- **Synthetic Minority Over-sampling Technique**\n",
        "\n",
        "- **Generates synthetic samples for minority class.**\n",
        "\n",
        "---\n",
        "\n",
        "## **25. Implementing SMOTE in Python**\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import SMOTE\n",
        "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
        "```\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Handling Outliers**\n",
        "\n",
        "## **26. What are outliers, and why do they matter?**\n",
        "\n",
        "- **Outliers** are extreme values that differ significantly from other observations.\n",
        "\n",
        "- **Impact:**\n",
        "  \n",
        "  - Skews the dataset.\n",
        "  \n",
        "  - Affects model performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **27. Methods to detect outliers**\n",
        "\n",
        "1. **Z-score Method**  \n",
        "\n",
        "   - Data points with |Z-score| > 3 are considered outliers.\n",
        "\n",
        "2. **IQR (Interquartile Range) Method**  \n",
        "\n",
        "   - Detects values below **Q1 - 1.5*IQR** or above **Q3 + 1.5*IQR**.\n",
        "\n",
        "3. **Visualization (Boxplots, Scatter Plots, Histograms)**\n",
        "\n",
        "---\n",
        "\n",
        "## **28. Example: Detecting outliers using IQR in Python**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Generate random data\n",
        "\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(50, 10, 100)  # Mean = 50, Std = 10\n",
        "\n",
        "# Convert to DataFrame\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"values\"])\n",
        "\n",
        "# Calculate Q1, Q3, and IQR\n",
        "\n",
        "Q1 = df[\"values\"].quantile(0.25)\n",
        "Q3 = df[\"values\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Find outliers\n",
        "\n",
        "outliers = df[(df[\"values\"] < (Q1 - 1.5 * IQR)) | (df[\"values\"] > (Q3 + 1.5 * IQR))]\n",
        "print(\"Outliers:\\n\", outliers)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **29. Impact of outliers on ML models**\n",
        "\n",
        "- **Linear models** (e.g., Linear Regression) are highly sensitive.\n",
        "\n",
        "- **Tree-based models** (e.g., Random Forest) are **less affected.**\n",
        "\n",
        "---\n",
        "\n",
        "## **30. Handling outliers using IQR method**\n",
        "\n",
        "```python\n",
        "# Remove outliers\n",
        "df_cleaned = df[(df[\"values\"] >= (Q1 - 1.5 * IQR)) & (df[\"values\"] <= (Q3 + 1.5 * IQR))]\n",
        "print(\"Cleaned Data:\\n\", df_cleaned)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Feature Extraction and Feature Scaling**\n",
        "\n",
        "## **31. What is feature extraction, and why is it important?**\n",
        "\n",
        "- **Feature extraction** transforms raw data into informative input for ML models.\n",
        "\n",
        "- **Example:** Extracting **edges** from images for facial recognition.\n",
        "\n",
        "---\n",
        "\n",
        "## **32. Difference between feature selection and feature extraction**\n",
        "\n",
        "| Feature Selection | Feature Extraction |\n",
        "|-------------------|-------------------|\n",
        "| Keeps existing features | Creates new features |\n",
        "| Example: Selecting top 5 features | PCA transforms 10 features into 3 |\n",
        "\n",
        "---\n",
        "\n",
        "## **33. What is feature scaling, and when should it be applied?**\n",
        "\n",
        "- **Ensures all features have the same scale.**\n",
        "\n",
        "- **Required for algorithms like KNN, SVM, and Logistic Regression.**\n",
        "\n",
        "---\n",
        "\n",
        "## **34. Standardization vs Normalization**\n",
        "\n",
        "| Method | Formula | When to use |\n",
        "|--------|---------|------------|\n",
        "| **Standardization** | (X - Mean) / Std Dev | For normally distributed data |\n",
        "| **Normalization** | (X - Min) / (Max - Min) | For bounded values (0 to 1) |\n",
        "\n",
        "---\n",
        "\n",
        "## **35. Implementing feature scaling using StandardScaler**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **36. Implementing MinMaxScaler in Python**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Data Encoding**\n",
        "\n",
        "## **37. What is data encoding, and why is it necessary?**\n",
        "\n",
        "- Converts categorical data into a numerical format.\n",
        "\n",
        "- Essential for models like Logistic Regression, SVM.\n",
        "\n",
        "---\n",
        "\n",
        "## **38. Label Encoding vs One-Hot Encoding**\n",
        "\n",
        "| Encoding | Description | Example |\n",
        "|----------|------------|---------|\n",
        "| **Label Encoding** | Assigns numbers to categories | Red â†’ 0, Blue â†’ 1 |\n",
        "| **One-Hot Encoding** | Creates binary columns for each category | Red â†’ [1, 0], Blue â†’ [0, 1] |\n",
        "\n",
        "---\n",
        "\n",
        "## **39. Implementing One-Hot Encoding in Python**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
        "df_encoded = pd.get_dummies(df, columns=['Color'])\n",
        "print(df_encoded)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **40. Label Encoding using sklearn**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['Color_Encoded'] = encoder.fit_transform(df['Color'])\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Hypothesis Testing**\n",
        "\n",
        "## **41. What is a hypothesis test?**\n",
        "\n",
        "- **Statistical method** to determine if an assumption about a dataset is true.\n",
        "\n",
        "- Uses **p-value** to accept or reject the **null hypothesis (Hâ‚€).**\n",
        "\n",
        "---\n",
        "\n",
        "## **42. Types of hypothesis tests**\n",
        "\n",
        "1. **Z-Test** â€“ When population variance is known.\n",
        "\n",
        "2. **T-Test** â€“ When population variance is unknown.\n",
        "\n",
        "3. **Chi-Square Test** â€“ For categorical data.\n",
        "\n",
        "4. **ANOVA** â€“ Compares multiple groups.\n",
        "\n",
        "---\n",
        "\n",
        "## **43. Example: One-sample Z-test in Python**\n",
        "\n",
        "```python\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "import numpy as np\n",
        "\n",
        "# Generate sample data\n",
        "\n",
        "np.random.seed(42)\n",
        "sample = np.random.normal(50, 10, 30)  # Mean = 50, Std = 10\n",
        "\n",
        "# Perform Z-test (H0: mean = 50)\n",
        "\n",
        "z_stat, p_value = ztest(sample, value=50)\n",
        "print(\"Z-statistic:\", z_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H0: The sample mean is significantly different from 50\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant difference\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Chi-Square Test**\n",
        "\n",
        "## **44. Performing a Chi-Square test in Python**\n",
        "\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Observed and expected frequencies\n",
        "\n",
        "observed = np.array([40, 60])\n",
        "expected = np.array([50, 50])\n",
        "\n",
        "# Chi-Square test\n",
        "\n",
        "chi2_stat, p_value = stats.chisquare(observed, expected)\n",
        "print(\"Chi-Square Statistic:\", chi2_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H0: Observed distribution is significantly different\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant difference\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ ANOVA (Analysis of Variance)**\n",
        "\n",
        "## **45. Performing a One-Way ANOVA Test**\n",
        "\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Generate sample data\n",
        "\n",
        "np.random.seed(42)\n",
        "group1 = np.random.normal(50, 10, 30)\n",
        "group2 = np.random.normal(55, 10, 30)\n",
        "group3 = np.random.normal(60, 10, 30)\n",
        "\n",
        "# Perform ANOVA\n",
        "\n",
        "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
        "print(\"F-statistic:\", f_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H0: At least one group mean is significantly different\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant difference among groups\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **Bayesian Inference**\n",
        "\n",
        "## **46. What is Bayesian inference?**\n",
        "\n",
        "- **Bayes' Theorem** is used to update probabilities based on new evidence.\n",
        "\n",
        "- **Formula:**\n",
        "  [\n",
        "  P(A | B) = {P(B | A) P(A)}{P(B)}\n",
        "  ]\n",
        "- **Example Application:** Spam detection, medical diagnosis.\n",
        "\n",
        "---\n",
        "\n",
        "## **47. Implementing Bayesian inference in Python**\n",
        "\n",
        "```python\n",
        "def bayes_theorem(prior_A, prob_B_given_A, prob_B):\n",
        "    return (prob_B_given_A * prior_A) / prob_B\n",
        "\n",
        "# Example: Medical Test (A = Disease, B = Positive Test)\n",
        "\n",
        "prior_A = 0.01  # Probability of having the disease\n",
        "prob_B_given_A = 0.9  # Test sensitivity (True Positive Rate)\n",
        "prob_B_given_not_A = 0.1  # False Positive Rate\n",
        "prob_B = (prob_B_given_A * prior_A) + (prob_B_given_not_A * (1 - prior_A))\n",
        "\n",
        "# Compute posterior probability\n",
        "\n",
        "posterior = bayes_theorem(prior_A, prob_B_given_A, prob_B)\n",
        "print(\"Posterior Probability (Having Disease | Positive Test):\", posterior)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **F-Test for Variance Comparison**\n",
        "\n",
        "## **48. What is an F-test?**\n",
        "\n",
        "- Compares variances of two datasets.\n",
        "\n",
        "- Used to test **homogeneity of variance** in ANOVA.\n",
        "\n",
        "---\n",
        "\n",
        "## **49. Performing an F-test in Python**\n",
        "\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Generate sample data\n",
        "\n",
        "np.random.seed(42)\n",
        "group1 = np.random.normal(50, 10, 30)\n",
        "group2 = np.random.normal(55, 15, 30)\n",
        "\n",
        "# Calculate variance\n",
        "\n",
        "var1 = np.var(group1, ddof=1)\n",
        "var2 = np.var(group2, ddof=1)\n",
        "\n",
        "# Compute F-statistic\n",
        "\n",
        "F_stat = var1 / var2\n",
        "p_value = stats.f.cdf(F_stat, len(group1)-1, len(group2)-1)\n",
        "\n",
        "print(\"F-statistic:\", F_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H0: Variances are significantly different\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant difference in variances\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Chi-Square Goodness-of-Fit Test**\n",
        "\n",
        "## **50. What is a goodness-of-fit test?**\n",
        "- Tests if observed data follows a specific expected distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## **51. Performing a Chi-Square Goodness-of-Fit Test**\n",
        "\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Observed and expected frequencies\n",
        "\n",
        "observed = np.array([30, 50, 20])\n",
        "expected = np.array([33, 33, 34])\n",
        "\n",
        "# Chi-Square test\n",
        "\n",
        "chi2_stat, p_value = stats.chisquare(observed, expected)\n",
        "print(\"Chi-Square Statistic:\", chi2_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H0: The observed distribution is different from the expected distribution\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant difference\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **Visualizing Probability Distributions**\n",
        "\n",
        "## **52. Visualizing the Standard Normal Distribution**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "y = stats.norm.pdf(x, 0, 1)  # Mean = 0, Std Dev = 1\n",
        "\n",
        "plt.plot(x, y, label=\"Standard Normal Distribution\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Probability Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **53. Visualizing the F-distribution**\n",
        "\n",
        "```python\n",
        "x = np.linspace(0, 5, 1000)\n",
        "y = stats.f.pdf(x, dfn=5, dfd=20)\n",
        "\n",
        "plt.plot(x, y, label=\"F-distribution (dfn=5, dfd=20)\")\n",
        "plt.xlabel(\"F-value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **ðŸ“Œ Z-Test for Comparing Proportions**\n",
        "\n",
        "## **54. Performing a Z-test for Proportions**\n",
        "\n",
        "```python\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Sample proportions\n",
        "\n",
        "count = np.array([50, 30])  # Successes in group1 and group2\n",
        "nobs = np.array([100, 100])  # Total observations in each group\n",
        "\n",
        "# Perform Z-test\n",
        "\n",
        "z_stat, p_value = sm.stats.proportions_ztest(count, nobs)\n",
        "print(\"Z-statistic:\", z_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H0: Proportions are significantly different\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant difference in proportions\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "Ntdk9QwZ-i_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Generate random data\n",
        "\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(50, 10, 100)  # Mean = 50, Std = 10\n",
        "\n",
        "# Convert to DataFrame\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"values\"])\n",
        "\n",
        "# Calculate Q1, Q3, and IQR\n",
        "\n",
        "Q1 = df[\"values\"].quantile(0.25)\n",
        "Q3 = df[\"values\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Find outliers\n",
        "\n",
        "outliers = df[(df[\"values\"] < (Q1 - 1.5 * IQR)) | (df[\"values\"] > (Q3 + 1.5 * IQR))]\n",
        "print(\"Outliers:\\n\", outliers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENe4V1vxrJGr",
        "outputId": "bcddbb3b-c8be-4090-830e-49fb3806f826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers:\n",
            "        values\n",
            "74  23.802549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Generate random data\n",
        "\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(50, 10, 100)  # Mean = 50, Std = 10\n",
        "\n",
        "# Convert to DataFrame\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"values\"])\n",
        "\n",
        "# Calculate Q1, Q3, and IQR\n",
        "\n",
        "Q1 = df[\"values\"].quantile(0.25)\n",
        "Q3 = df[\"values\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Find outliers\n",
        "\n",
        "outliers = df[(df[\"values\"] < (Q1 - 1.5 * IQR)) | (df[\"values\"] > (Q3 + 1.5 * IQR))]\n",
        "print(\"Outliers:\\n\", outliers)\n",
        "\n",
        "# --- Code from the second cell ---\n",
        "\n",
        "# Remove outliers\n",
        "df_no_outliers = df[(df[\"values\"] >= (Q1 - 1.5 * IQR)) & (df[\"values\"] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "# Print the DataFrame without outliers\n",
        "print(\"\\nDataFrame without outliers:\\n\", df_no_outliers)\n",
        "\n",
        "# Example of a simple linear regression (you would typically use scikit-learn)\n",
        "# Assuming you have a target variable (replace 'target' with your actual column name)\n",
        "# and you want to predict 'values' based on 'target'.\n",
        "\n",
        "# This is just a placeholder; real regression would use a dedicated library like scikit-learn.\n",
        "# For demonstration, we're using a simplified calculation.\n",
        "\n",
        "if 'target' in df.columns:  # check if 'target' exists before using it\n",
        "    slope = np.cov(df[\"values\"], df[\"target\"])[0, 1] / np.var(df[\"target\"])\n",
        "    intercept = np.mean(df[\"values\"]) - slope * np.mean(df[\"target\"])\n",
        "    print(\"\\nSimple linear regression (example):\")\n",
        "    print(f\"Slope: {slope}\")\n",
        "    print(f\"Intercept: {intercept}\")\n",
        "\n",
        "    # Example Prediction\n",
        "    new_target_value = 60  # Replace with a new target value\n",
        "    predicted_value = slope * new_target_value + intercept\n",
        "    print(f\"Predicted value for target={new_target_value}: {predicted_value}\")\n",
        "else:\n",
        "    print(\"\\n'target' column not found in the DataFrame. Skipping linear regression example.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7whUZHWTbpWk",
        "outputId": "a60ee63a-1c31-428a-888d-9e7d0955cde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers:\n",
            "        values\n",
            "74  23.802549\n",
            "\n",
            "DataFrame without outliers:\n",
            "        values\n",
            "0   54.967142\n",
            "1   48.617357\n",
            "2   56.476885\n",
            "3   65.230299\n",
            "4   47.658466\n",
            "..        ...\n",
            "95  35.364851\n",
            "96  52.961203\n",
            "97  52.610553\n",
            "98  50.051135\n",
            "99  47.654129\n",
            "\n",
            "[99 rows x 1 columns]\n",
            "\n",
            "'target' column not found in the DataFrame. Skipping linear regression example.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression and ML Interview Questions with Answers**\n",
        "\n",
        "### **Q: What is Simple Linear Regression, and how is it used in predictive modeling?**\n",
        "**A:** Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X). It is used in predictive modeling to estimate or predict the value of Y for a given value of X by fitting a straight line (Y = mX + c).\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Explain the mathematical equation of Simple Linear Regression.**\n",
        "**A:** The equation is:\n",
        "\\[\n",
        "Y = mX + c\n",
        "\\]\n",
        "Where:\n",
        "- **Y** is the dependent variable (predicted value),\n",
        "- **X** is the independent variable,\n",
        "- **m** is the slope (rate of change),\n",
        "- **c** is the intercept (value of Y when X = 0).\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What is the significance of the slope and intercept in the linear regression equation?**\n",
        "**A:**  \n",
        "- The **slope (m)** tells how much Y changes for each unit increase in X.  \n",
        "- The **intercept (c)** is the predicted value of Y when X = 0.  \n",
        "Together, they define the regression line.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: How would you visualize the relationship between variables using a linear regression model?**\n",
        "**A:** By plotting a **scatter plot** of the data points and adding a **regression line** (best-fit line). This line shows the direction and strength of the relationship between X and Y.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What assumptions must hold true for Simple Linear Regression to work effectively?**\n",
        "**A:**  \n",
        "1. **Linearity**: The relationship between X and Y is linear.  \n",
        "2. **Independence**: Observations are independent.  \n",
        "3. **Homoscedasticity**: Constant variance of errors.  \n",
        "4. **Normality**: Residuals are normally distributed.  \n",
        "5. **No significant outliers**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What are the limitations of using Simple Linear Regression for prediction?**\n",
        "**A:**  \n",
        "- Assumes linearity (not suitable for non-linear relationships).  \n",
        "- Sensitive to outliers.  \n",
        "- Canâ€™t handle multiple influencing variables.  \n",
        "- Predicts poorly when assumptions are violated.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Explain how outliers can affect the performance of a Simple Linear Regression model.**\n",
        "**A:**  \n",
        "Outliers can distort the slope and intercept of the regression line, leading to **biased coefficients** and **inaccurate predictions**. They can heavily influence the model if not detected and handled properly.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: How would you evaluate the goodness of fit for a Simple Linear Regression model?**\n",
        "**A:**  \n",
        "- **RÂ² (Coefficient of Determination)**: Measures how much variance in Y is explained by X.  \n",
        "- **Residual plots**: Check for patterns to confirm model validity.  \n",
        "- **MSE, RMSE, MAE**: Error metrics to quantify prediction accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "Great! Letâ€™s continue with the next set of **Multiple Linear Regression (MLR)** and related interview questions with detailed answers:\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What is the difference between Simple Linear Regression and Multiple Linear Regression?**\n",
        "**A:**  \n",
        "- **Simple Linear Regression** involves **one independent variable** predicting a dependent variable.  \n",
        "- **Multiple Linear Regression** uses **two or more independent variables** to predict a single dependent variable.  \n",
        "> MLR captures more complex, multi-factor relationships compared to SLR.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What are the assumptions underlying Multiple Linear Regression?**\n",
        "**A:**  \n",
        "1. **Linearity**: Linear relationship between predictors and outcome.  \n",
        "2. **Independence of errors**: Observations are independent.  \n",
        "3. **Homoscedasticity**: Constant variance of residuals.  \n",
        "4. **Normal distribution of residuals**.  \n",
        "5. **No multicollinearity** among predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Explain how feature selection is important in Multiple Linear Regression models.**\n",
        "**A:**  \n",
        "Feature selection helps:  \n",
        "- Reduce **overfitting**  \n",
        "- Improve **model interpretability**  \n",
        "- Enhance **training efficiency**  \n",
        "- Eliminate **irrelevant or redundant predictors**  \n",
        "Techniques include forward/backward selection, Recursive Feature Elimination (RFE), and Lasso.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What techniques can be used to handle multicollinearity in Multiple Linear Regression?**\n",
        "**A:**  \n",
        "- **Remove one of the correlated variables**  \n",
        "- Use **Principal Component Analysis (PCA)**  \n",
        "- Apply **Ridge or Lasso Regression**  \n",
        "- Check **Variance Inflation Factor (VIF)** and drop variables with high VIF values.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: How do you interpret the coefficients in a Multiple Linear Regression model?**\n",
        "**A:**  \n",
        "Each coefficient represents the **change in the dependent variable (Y)** for a **one-unit increase in that predictor**, **holding all other predictors constant**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What does the p-value indicate in the context of feature importance in regression models?**\n",
        "**A:**  \n",
        "- A **low p-value (< 0.05)** indicates that the predictor is **statistically significant** in explaining the variation in Y.  \n",
        "- A **high p-value** suggests the variable might not contribute meaningfully.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What is the significance of adjusted R-squared in Multiple Linear Regression?**\n",
        "**A:**  \n",
        "Adjusted RÂ² accounts for the **number of predictors** in the model. Unlike RÂ², it only increases if the new variable actually **improves the model**, preventing misleading interpretations from irrelevant features.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: How would you rank features based on importance in a Multiple Linear Regression model?**\n",
        "**A:**  \n",
        "- Based on **standardized coefficients** (beta values)  \n",
        "- **P-values** of features  \n",
        "- Feature selection methods like **RFE** or using **model coefficients** after normalization\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What is R-squared, and what does it indicate in a regression model?**\n",
        "**A:**  \n",
        "RÂ² represents the **proportion of variance in the dependent variable explained by the independent variables**.  \n",
        "- **RÂ² = 1** â†’ Perfect prediction  \n",
        "- **RÂ² = 0** â†’ Model explains nothing\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Explain the difference between R-squared and adjusted R-squared.**\n",
        "**A:**  \n",
        "- **RÂ²** increases with more variables, even if theyâ€™re irrelevant.  \n",
        "- **Adjusted RÂ²** penalizes unnecessary predictors and gives a **more reliable measure** when comparing models with different numbers of variables.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Can R-squared be negative, and if so, under what circumstances?**\n",
        "**A:**  \n",
        "Yes. RÂ² can be negative when the model **fits the data worse than a horizontal mean line**. This often indicates a **poor or inappropriate model**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Why is adjusted R-squared preferred when comparing models with different numbers of predictors?**\n",
        "**A:**  \n",
        "Because it adjusts for the **number of features**, helping you avoid models that just \"look good\" by adding useless variables. It rewards only **truly impactful predictors**.\n",
        "\n",
        "---\n",
        "Awesome! Letâ€™s continue with the **Polynomial Regression**, **Model Evaluation Metrics**, and **ML Pipeline** interview questions with detailed answers:\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”º **Polynomial Regression Questions**\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What is Polynomial Regression, and how does it extend Simple Linear Regression?**\n",
        "**A:**  \n",
        "Polynomial Regression is a form of regression that models the relationship between the independent variable (X) and the dependent variable (Y) as an **nth-degree polynomial**:\n",
        "\\[\n",
        "Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n\n",
        "\\]\n",
        "It captures **non-linear patterns** that simple linear regression cannot.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Explain the main difference between Linear and Polynomial Regression.**\n",
        "**A:**  \n",
        "- **Linear Regression** fits a **straight line** to the data.\n",
        "- **Polynomial Regression** fits a **curved line** by introducing **higher-degree terms** (e.g., \\(X^2, X^3\\)) of the predictor variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What are the challenges of using Polynomial Regression for high-degree polynomials?**\n",
        "**A:**  \n",
        "- **Overfitting** the data  \n",
        "- **Poor generalization** to new data  \n",
        "- **Numerical instability**  \n",
        "- **Difficult interpretation** of complex curves\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: How would you choose the degree of the polynomial in a Polynomial Regression model?**\n",
        "**A:**  \n",
        "Use:\n",
        "- **Cross-validation**\n",
        "- **Learning curves**\n",
        "- **Error metrics** (MSE, RMSE)\n",
        "- **Visual inspection** of the fitted curve vs actual data\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Š **Model Evaluation Metrics**\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What are MSE, MAE, and RMSE, and how are they used to evaluate regression models?**\n",
        "**A:**  \n",
        "- **MAE (Mean Absolute Error)**: Average of absolute errors  \n",
        "- **MSE (Mean Squared Error)**: Average of squared errors  \n",
        "- **RMSE (Root Mean Squared Error)**: Square root of MSE  \n",
        "\n",
        "These metrics measure **how far predictions are from actual values**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: How do MSE, MAE, and RMSE differ in terms of their sensitivity to outliers?**\n",
        "**A:**  \n",
        "- **MAE** is **robust to outliers** (treats all errors equally)  \n",
        "- **MSE** and **RMSE** are **more sensitive** because errors are **squared**, magnifying large deviations\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Which of the threeâ€”MSE, MAE, or RMSEâ€”is most commonly used, and why?**\n",
        "**A:**  \n",
        "**RMSE** is commonly used because:\n",
        "- It **penalizes larger errors** more (like MSE)\n",
        "- It retains the **same units as the target variable**, making it interpretable\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: How would you interpret a low RMSE but high MAE for a regression model?**\n",
        "**A:**  \n",
        "This suggests:\n",
        "- Some **predictions are very accurate** (low RMSE)\n",
        "- But **others have larger absolute errors** (high MAE)\n",
        "This means the model may perform **inconsistently** across different data points.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ› ï¸ **Machine Learning Pipeline**\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What is an ML pipeline, and why is it important for machine learning workflows?**\n",
        "**A:**  \n",
        "An **ML pipeline** is a sequence of steps that automate the **data preprocessing**, **model training**, **evaluation**, and **deployment** process. It ensures **reproducibility**, **consistency**, and **scalability** in ML projects.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: How would you build an end-to-end Machine Learning pipeline in Python?**\n",
        "**A:**  \n",
        "You can use `scikit-learn`'s `Pipeline` module:\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "pipe.fit(X_train, y_train)\n",
        "```\n",
        "Include: data cleaning, encoding, scaling, feature selection, model, and evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What are the key components of an ML pipeline, and how do they interact?**\n",
        "**A:**  \n",
        "1. **Data Preprocessing**: Cleaning, missing values, encoding  \n",
        "2. **Feature Engineering**: Scaling, transformation  \n",
        "3. **Model Selection & Training**  \n",
        "4. **Evaluation**: Metrics like RÂ², RMSE  \n",
        "5. **Deployment (optional)**\n",
        "\n",
        "Each step passes its output as input to the next step, creating a streamlined workflow.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Explain how you can use Scikit-learn to create an ML pipeline.**\n",
        "**A:**  \n",
        "Use `Pipeline` or `make_pipeline` to chain operations like scaling and model fitting:\n",
        "```python\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "pipe = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n",
        "pipe.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "This ensures all preprocessing is applied consistently during both training and prediction.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2ySWTUgVPTD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest versiond\n",
        "path = kagglehub.dataset_download(\"charlottebennett1234/lifestyle-factors-and-their-impact-on-students\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsGVYkH6HKWG",
        "outputId": "72694465-2cec-42bd-a65b-4ed2299bb069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/lifestyle-factors-and-their-impact-on-students\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kLidmApDHI58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"itsandrewxd/pokmon-platinum-exp-and-leveling-analysis-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4jwUdQGM9o9",
        "outputId": "11dd6622-2be5-4906-df3f-773086bce7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/itsandrewxd/pokmon-platinum-exp-and-leveling-analysis-dataset?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86.2k/86.2k [00:00<00:00, 50.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/itsandrewxd/pokmon-platinum-exp-and-leveling-analysis-dataset/versions/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "C_H-w5FmN-_8",
        "outputId": "be5434f6-ca6b-49ba-d1dc-0d56a008ec40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb10fb09-01cb-4499-a083-442fc0768812\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eb10fb09-01cb-4499-a083-442fc0768812\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pokemon_data.csv to pokemon_data (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "Pokemon_data_df = pd.read_csv(\"pokemon_data.csv\")\n",
        "\n",
        "print(Pokemon_data_df)\n",
        "\n",
        "df_null = Pokemon_data_df.isnull().sum()\n",
        "\n",
        "print(df_null)\n",
        "\n",
        "df_duplicate = Pokemon_data_df.duplicated().sum()\n",
        "\n",
        "print(df_duplicate)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_6opEZqQhZ_",
        "outputId": "5c8a96e1-b30e-479e-df2d-b3cc589d6b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Number                     Name           Type  Total   HP  Attack  \\\n",
            "0          1                Bulbasaur   Grass Poison    318   45      49   \n",
            "1          2                  Ivysaur   Grass Poison    405   60      62   \n",
            "2          3                 Venusaur   Grass Poison    525   80      82   \n",
            "3          3   Venusaur Mega Venusaur   Grass Poison    625   80     100   \n",
            "4          4               Charmander           Fire    309   39      52   \n",
            "...      ...                      ...            ...    ...  ...     ...   \n",
            "1210    1023               Iron Crown  Steel Psychic    590   90      72   \n",
            "1211    1024    Terapagos Normal Form         Normal    450   90      65   \n",
            "1212    1024  Terapagos Terastal Form         Normal    600   95      95   \n",
            "1213    1024   Terapagos Stellar Form         Normal    700  160     105   \n",
            "1214    1025                Pecharunt   Poison Ghost    600   88      88   \n",
            "\n",
            "      Defense  Sp. Atk  Sp. Def  Speed  \n",
            "0          49       65       65     45  \n",
            "1          63       80       80     60  \n",
            "2          83      100      100     80  \n",
            "3         123      122      120     80  \n",
            "4          43       60       50     65  \n",
            "...       ...      ...      ...    ...  \n",
            "1210      100      122      108     98  \n",
            "1211       85       65       85     60  \n",
            "1212      110      105      110     85  \n",
            "1213      110      130      110     85  \n",
            "1214      160       88       88     88  \n",
            "\n",
            "[1215 rows x 10 columns]\n",
            "Number     0\n",
            "Name       0\n",
            "Type       0\n",
            "Total      0\n",
            "HP         0\n",
            "Attack     0\n",
            "Defense    0\n",
            "Sp. Atk    0\n",
            "Sp. Def    0\n",
            "Speed      0\n",
            "dtype: int64\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])  # Feature\n",
        "y = np.array([2, 4, 5, 4, 5])           # Target\n",
        "\n",
        "# Model training\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Prediction\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print(f\"Coefficient: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyytBqDPNIGe",
        "outputId": "9557be28-0467-4dad-b7b2-22e17d922657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient: [0.6]\n",
            "Intercept: 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Add intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit model\n",
        "model = sm.OLS(y, X).fit()\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cReV1L6oOcIW",
        "outputId": "dd57793b-50b6-4aa8-d587-61e172b31c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.600\n",
            "Model:                            OLS   Adj. R-squared:                  0.467\n",
            "Method:                 Least Squares   F-statistic:                     4.500\n",
            "Date:                Wed, 30 Apr 2025   Prob (F-statistic):              0.124\n",
            "Time:                        15:03:59   Log-Likelihood:                -5.2598\n",
            "No. Observations:                   5   AIC:                             14.52\n",
            "Df Residuals:                       3   BIC:                             13.74\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          2.2000      0.938      2.345      0.101      -0.785       5.185\n",
            "x1             0.6000      0.283      2.121      0.124      -0.300       1.500\n",
            "==============================================================================\n",
            "Omnibus:                          nan   Durbin-Watson:                   2.017\n",
            "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.570\n",
            "Skew:                           0.289   Prob(JB):                        0.752\n",
            "Kurtosis:                       1.450   Cond. No.                         8.37\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
            "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "data = pd.DataFrame({\n",
        "    'Experience': [1, 2, 3, 4, 5],\n",
        "    'Age': [22, 25, 28, 30, 35],\n",
        "    'Salary': [30000, 35000, 50000, 55000, 60000]\n",
        "})\n",
        "\n",
        "X = data[['Experience', 'Age']]\n",
        "y = data['Salary']\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sDe12S5Oe-B",
        "outputId": "d4e9e139-a132-4c13-c898-af68552c3803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [12894.73684211 -1578.94736842]\n",
            "Intercept: 51526.31578947367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "model = LinearRegression()\n",
        "selector = RFE(model, n_features_to_select=1)\n",
        "selector = selector.fit(X, y)\n",
        "print(selector.support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK8ZH_3COj3e",
        "outputId": "d3de082a-af27-460d-fd3f-667c4ce94b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "scores = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2')\n",
        "print(f\"RÂ² scores: {scores}\")\n",
        "print(f\"Average RÂ²: {scores.mean()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI7GTBjQOonm",
        "outputId": "2a37be15-b99e-4118-9b41-6d4292d4ad8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RÂ² scores: [nan nan nan nan nan]\n",
            "Average RÂ²: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Load\n",
        "with open('model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n"
      ],
      "metadata": {
        "id": "m4PudjfIOxli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample Data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 6, 14, 28, 45])\n",
        "\n",
        "# Polynomial Model (degree 2)\n",
        "poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "poly_model.fit(X, y)\n",
        "\n",
        "print(poly_model.named_steps['linearregression'].coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW092YptP2nP",
        "outputId": "6f00bfab-b9f1-4ac1-b096-c574405ccca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.         -2.91428571  2.28571429]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "\n",
        "# Example\n",
        "df = pd.DataFrame({\n",
        "    'X1': [1, 2, 3, 4, 5],\n",
        "    'X2': [2, 4, 6, 8, 10],  # Perfectly correlated with X1\n",
        "    'X3': [5, 3, 6, 9, 2]\n",
        "})\n",
        "\n",
        "# Calculate VIF\n",
        "X = sm.add_constant(df)\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = X.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9vzCVjVP7ry",
        "outputId": "bf23f111-8b54-4449-f404-8b22731ac137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Feature       VIF\n",
            "0   const  9.666667\n",
            "1      X1       inf\n",
            "2      X2       inf\n",
            "3      X3  1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  vif = 1. / (1. - r_squared_i)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpW6SapEQMge"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}